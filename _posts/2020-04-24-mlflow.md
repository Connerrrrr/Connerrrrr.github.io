---
layout: post
title: Integration between MLflow and Kubernetes
date: 2020-04-24
author: Conner
tags: [Kubernetes, MLflow, Deployment, ML lifecycle]
comments: false
toc: true
pinned: false
---

## Background

As Azure machine learning service is integrated with MLOps, which does not offer powerful model registry as MLflow does, it is required to design customized architecture to host MLflow central server. Meanwhile, kubernetes has consolidated its popularity as a portable, extensible, open-source platform. It is feasible to put everything on Azure Kubernetes Service.

## Architecture Design

**Note: Lucidchart is embeded, incognito mode may cause problems.**

<div style="width: 640px; height: 480px; margin: 10px; position: relative;"><iframe allowfullscreen frameborder="0" style="width:640px; height:480px" src="https://lucid.app/documents/embeddedchart/dc9bb985-9690-4701-87c9-26ca8455514f" id="TiQVJAstzdxG"></iframe></div>

## Explaination

1. Pulling raw data from data lake
2. Training the models with PySpark in Databricks on Azure, which is integrated with Azure Machine Learning Service
3. Packing the model into the docker image along with the code injected for communicating with MLFlow central server
4. Pushing the image to Azure Container Registry
5. Deploying the image to Azure k8s Service with kube-cli through configured YAML file
6. Tracking and registering the existing models using MLFlow Tracking Server that was already deployed as a service
7. Exposing Machine Learning deployments as a single service to Azure k8s Load Balancer Endpoint with k8s traffic rules
8. Feeding Real-time data or batch-inference data from data lake to deployed services
9. Exposing Machine Learning API to the public network
